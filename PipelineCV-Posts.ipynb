{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicjalizacja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/mc/Programs/anaconda/envs/tfdeeplearning/lib/python3.5/site-packages\n",
      "Requirement already satisfied: nltk>=3.1 in /home/mc/Programs/anaconda/envs/tfdeeplearning/lib/python3.5/site-packages (from textblob)\n",
      "Requirement already satisfied: six in /home/mc/Programs/anaconda/envs/tfdeeplearning/lib/python3.5/site-packages (from nltk>=3.1->textblob)\n",
      "[nltk_data] Downloading package brown to /home/mc/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator\n",
    "from pyspark.ml.param import Param\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import floor, rand, udf, max\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import json\n",
    "\n",
    "from post_extractor.posts import (\n",
    "    PostTransformer,\n",
    "    TranslateTransformer,\n",
    "    BasicSpeechPartsTransformer\n",
    ")\n",
    "\n",
    "sconf = SparkConf()              \\\n",
    "    .setMaster('local[*]')       \\\n",
    "    .setAppName('PipelineFlow')\n",
    "\n",
    "sc = SparkContext.getOrCreate(sconf)\n",
    "sess = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytywanie plików"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_posts(spark_ctx, files):\n",
    "    rdd = spark_ctx.wholeTextFiles(files)\n",
    "    rdd = rdd.map(lambda x: (x[0], json.loads(x[1])))\n",
    "    df = rdd.toDF(['file', 'content'], sampleRatio=0.2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykład zastosowania TransformerProxy do automatyzacji ewaluacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformerProxy` jest obiektem który opakowuje inny Transformer. Dzięki temu, możliwe jest stworzenie uniwersalnego pipeline'u (jak na rysunku poniżej) bez specyfikowania od razu konkretnych implementacji poszczególnych etapów. Np. wiemy, że pierwszy etap parsuje plik html dzieląc go na słowa, drugi etap usuwa obrazki a trzeci rozpoznaje i zlicza czasowniki, ale nie wiemy jakie konkretne implementacje będziemy chcieli dostarczyć dla poszczególnych etapów.\n",
    " \n",
    "W szczególności jeśli bedziemy chcieli mieć wiele różnych implementacji dla tego samego etapu opisane podejście będzie użyteczne. Testowanie takiego przepływu będzie odbywało się za pomocą klasy CrossValidator która zostanie opisana później. Na razie wspomnijmy jedynie o tym, że CrossValidator nie potrafi modyfikować pipeline'u poprzez zamianę np. jednego transformera na drugi, potrafi natomiast modyfikować parametry kolejnych etapów przepływu. Dzięki zastosowanemu podejściu CrossValidator będzie w stanie testować kombinację różnych implementacji poszególnych etapów pipeline'u.\n",
    "\n",
    "![title](pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasa TransformerProxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseVectorTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    def __init__(self):\n",
    "        super(DenseVectorTransformer, self).__init__()\n",
    "    def _transform(self, dataset):\n",
    "        toDenseVector = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "        return dataset.withColumn(self.getOutputCol(), toDenseVector(self.getInputCol()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransformerProxy(Transformer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerProxy, self).__init__()\n",
    "        self.transformer = Param(self, \"transformer\", \"\")\n",
    "\n",
    "    def set_transformer(self, transformer):\n",
    "        self._paramMap[self.transformer] = transformer\n",
    "        return self\n",
    "\n",
    "    def get_transformer(self):\n",
    "        return self.getOrDefault(self.transformer)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return self.get_transformer().transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EstimatorProxy(Estimator):\n",
    "    def __init__(self):\n",
    "        super(EstimatorProxy, self).__init__()\n",
    "        self.estimator = Param(self, \"estimator\", \"\")\n",
    "\n",
    "    def set_estimator(self, estimator):\n",
    "        self._paramMap[self.estimator] = estimator\n",
    "        return self\n",
    "\n",
    "    def get_estimator(self):\n",
    "        return self.getOrDefault(self.estimator)\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        return self.get_estimator().fit(dataset)\n",
    "    \n",
    "def create_estimators_grid(estimator, param_grid):\n",
    "    result = []\n",
    "    for param_map in param_grid:\n",
    "        est_copy = estimator.copy()\n",
    "        est_copy.setParams(**param_map)\n",
    "        result.append(est_copy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utworzenie instancji transformerów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przygotowanym pipeline możemy wykorzystać w pierwszym TransformerProxy trzy różne implementacje: \n",
    "- MeanFeaturesTransformer \n",
    "- MedianFeaturesTransformer \n",
    "- NumerOfOccurrencesFeaturesTransformer \n",
    "\n",
    "Możemy je zatem przekazać do abstrakcji ParamGridBuilder'a, który będzie parametrem przekazanym do klasy CrossValidator.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '    Klasa TranslateTransformer dziedziczy po klasach pyspark.ml.Transformer, '\n",
      " 'pyspark.ml.param.shared.HasInputCol,\\n'\n",
      " '    pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która '\n",
      " 'przyjmuje na wejściu obiekt typu dataframe.\\n'\n",
      " '    Metoda ta tłumaczy tekst zawarty w kolumnie inputCol  z języka polskiego '\n",
      " 'na angielski i umieszcza go w kolumnie\\n'\n",
      " '    outputCol.\\n'\n",
      " '    ')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(TranslateTransformer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Klasa BasicSpeechPartsTransformer dziedziczy  po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol,\n",
      "    pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe.\n",
      "    Metoda ta z tekstu zawartego w kolumnie inputCol zlicza wystąpienie podstawowych części mowy (rzeczownik, czasownik, przymiotnik)\n",
      "    i wstawia do outputCol w postaci tablicy wartości.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(BasicSpeechPartsTransformer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poster = PostTransformer()\n",
    "poster.setInputCol('content').setOutputCol('posts')\n",
    "\n",
    "translator = TranslateTransformer()\n",
    "translator.setInputCol('posts').setOutputCol('translated')\n",
    "\n",
    "speech_parter = BasicSpeechPartsTransformer()\n",
    "speech_parter.setInputCol('translated').setOutputCol('speech_parts')\n",
    "\n",
    "dv_transformer = DenseVectorTransformer()\n",
    "dv_transformer.setInputCol('speech_parts').setOutputCol('features')\n",
    "\n",
    "est_proxy = EstimatorProxy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie modyfikowalnego pipeline'u\n",
    "W tej wersji, wszystkie istniejące wczesniej stage zastępujemy obiektami `TransformerProxy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameterized_pipeline = Pipeline(stages=[\n",
    "    poster,\n",
    "    translator,\n",
    "    speech_parter,\n",
    "    dv_transformer,\n",
    "    est_proxy\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_posts(sc, \"data/posts/*\")\n",
    "dataWithLabels = data.withColumn('label', floor(rand() * 3).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossValidator uwzględniając wszystkie kombinacje dostarczonych parametrów wskazuje który zestaw parametrów cechuje się najlepszymi wynikami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CrossValidator will automatically find the best set of parameters\n",
    "\n",
    "dtParamGrid = ParamGridBuilder() \\\n",
    "  .baseOn({\"labelCol\": 'label'}) \\\n",
    "  .addGrid(\"maxDepth\", [2, 3]) \\\n",
    "  .addGrid(\"maxBins\", [5, 10]) \\\n",
    "  .build()\n",
    "\n",
    "estimators_grid = create_estimators_grid(DecisionTreeClassifier(), dtParamGrid)\n",
    "\n",
    "lrParamGrid = ParamGridBuilder() \\\n",
    "  .baseOn({\"labelCol\": 'label'}) \\\n",
    "  .addGrid(\"maxIter\", [10, 50]) \\\n",
    "  .addGrid(\"regParam\", [0, 0.1]) \\\n",
    "  .build()\n",
    "\n",
    "estimators_grid += create_estimators_grid(LogisticRegression(), lrParamGrid)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='label')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(est_proxy.estimator, estimators_grid) \\\n",
    "  .build()\n",
    "\n",
    "cv = CrossValidator(estimator=parameterized_pipeline,\n",
    "                   estimatorParamMaps=paramGrid,\n",
    "                   evaluator=evaluator,\n",
    "                   numFolds=3)\n",
    "cv_result = cv.fit(dataWithLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wypisanie nazw transformerow wybranych przez CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = cv_result.transform(data)\n",
    "selected = prediction.select(\"file\", \"speech_parts\", \"probability\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(file='file:/home/mc/Projects/ZTNBD-ZAD/data/posts/kascysko.blogspot.com.146.json', speech_parts=[79, 54, 30], probability=DenseVector([0.0, 0.0, 1.0]), prediction=2.0)\n",
      "Row(file='file:/home/mc/Projects/ZTNBD-ZAD/data/posts/kascysko.blogspot.com.142.json', speech_parts=[94, 83, 33], probability=DenseVector([0.0, 0.0, 1.0]), prediction=2.0)\n",
      "Row(file='file:/home/mc/Projects/ZTNBD-ZAD/data/posts/kascysko.blogspot.com.118.json', speech_parts=[48, 43, 17], probability=DenseVector([1.0, 0.0, 0.0]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen estimator:  DecisionTreeClassificationModel\n",
      "{\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-cacheNodeIds: false,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-checkpointInterval: 10,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-featuresCol: features,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-impurity: gini,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-labelCol: label,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-maxBins: 5,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-maxDepth: 2,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-maxMemoryInMB: 256,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-minInfoGain: 0.0,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-minInstancesPerNode: 1,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-predictionCol: prediction,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-probabilityCol: probability,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-rawPredictionCol: rawPrediction,\n",
      "\tDecisionTreeClassifier_4f8492d581eddf572fd9-seed: 6832503294723518452\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "estimator = cv_result.bestModel.stages[-1]\n",
    "print(\"Chosen estimator: \", type(estimator).__name__)\n",
    "print(estimator._java_obj.extractParamMap().toString().split(r\"\\n\\t\")[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "name": "PipelineCV_nofilter",
  "notebookId": 428779215948259
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
