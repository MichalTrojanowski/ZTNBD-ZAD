{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja przykładowego modułu do wczytywania danych\n",
    "W przyszłości takie moduły powinny być pisane przez drugą grupę i załączane wraz z innymi modułami. W przyszłości ścieżki w takich modułach mogą być zaszyte w kodzie samego modułu. Lokalizacja danych wejściowych będzie także wskazywać na pliki zlokalizowane na HDFS, a nie w katalogu roboczym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    key = 'key'\n",
    "    content = 'content'\n",
    "    path = 'path'\n",
    "    \n",
    "    def __init__(self, sc, rootDir):\n",
    "        self.sc = sc\n",
    "        self.rootDir = rootDir\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataframe):\n",
    "        def extractKey(entry):\n",
    "            fpath, content = entry\n",
    "            fname = fpath.split('/')[-1]\n",
    "            key = re.sub('\\.json$', '', fname)\n",
    "            return (key, json.loads(content))\n",
    "        \n",
    "        mappings = self.sc.textFile(self.rootDir + 'mapping.csv')        \\\n",
    "            .map(lambda line: line.split('|'))                      \\\n",
    "            .toDF([self.path, self.key])\n",
    "        \n",
    "        statements = self.sc.wholeTextFiles(self.rootDir + 'statement/') \\\n",
    "            .map(extractKey)                                        \\\n",
    "            .toDF([self.key, self.content])\n",
    "    \n",
    "        return mappings.join(statements, 'key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaimportowanie modułów przygotowanych przez drugą grupę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from external.modules.posts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - uruchomienie Sparka\n",
    "Domyślnie cały proces uruchamiany jest bez danych, pierwszy moduł w przepływie powinien być odpowiedzialny za ich wczytanie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sconf = SparkConf()              \\\n",
    "    .setMaster('local[*]')       \\\n",
    "    .setAppName('PipelineFlow')\n",
    "\n",
    "sc = SparkContext.getOrCreate(sconf)\n",
    "sql = SQLContext(sc)\n",
    "sess = SparkSession(sc)\n",
    "\n",
    "df = sql.createDataFrame(sc.emptyRDD(), StructType([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Defnicja przepływu*\n",
    "Przepływ definiowany jest poprzez stworzenie instancji kolejnych metod i przekazanie im kolumny (bądź kolumn) wejściowej i specyfikacja wyjścia każdej z metod. Następnie tworzony jest obiekt `pipeline` grupujący kolejne transformery w przepływ. Istotne jest aby zachować odpowiednią kolejność podczas przekazywania metod do pipeline, powinna ona być zgodna z zależnościami między metodami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoaderTransformer(sc, '/home/jovyan/work/kascysko.blogspot.com/')\n",
    "\n",
    "poster = PostTransformer()                  \\\n",
    "    .setInputCol(loader.content)            \\\n",
    "    .setOutputCol('posts')\n",
    "\n",
    "translator = TranslateTransformer()         \\\n",
    "    .setInputCol(poster.getOutputCol())     \\\n",
    "    .setOutputCol('translated')\n",
    "        \n",
    "sentencer = SentenceTransformer()           \\\n",
    "    .setInputCol(translator.getOutputCol()) \\\n",
    "    .setOutputCol('sentences')              \n",
    "\n",
    "pipeline = Pipeline(stages=[loader, poster, translator, sentencer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uruchomienie przepływu i odczyt danych\n",
    "Po dopasowaniu parametrów metod składających się na przepływ otrzymujemy model, który może być później zapisany dzieki czemu następnym razem nie będzie konieczności dobierania parametrów dla nowych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(df)\n",
    "output = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I Jantar spiced with yeast beer :) described wcierka for me a bit too expensive',\n",
       " 'I have banyan water with isany, but on holiday I am not very well with regularity.',\n",
       " 'Jantar is waiting for autumn.',\n",
       " 'I have this wcierkę on wishliscie but is piekielnie droga: /',\n",
       " 'I like to use all wcierki :)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.first().sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmiana przepływu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counter = SpeechPartsTransformer()       \\\n",
    "    .setInputCol(translator.getOutputCol())  \\\n",
    "    .setOutputCol('pos_count')\n",
    "\n",
    "pipeline_alt = Pipeline(stages=[loader, poster, translator, pos_counter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_alt = pipeline_alt.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CC': 28,\n",
       " 'CD': 6,\n",
       " 'DT': 69,\n",
       " 'EX': 2,\n",
       " 'IN': 106,\n",
       " 'JJ': 57,\n",
       " 'JJR': 3,\n",
       " 'JJS': 2,\n",
       " 'MD': 19,\n",
       " 'NN': 137,\n",
       " 'NNP': 23,\n",
       " 'NNS': 32,\n",
       " 'PDT': 1,\n",
       " 'PRP': 106,\n",
       " 'PRP$': 12,\n",
       " 'RB': 66,\n",
       " 'RBR': 3,\n",
       " 'RBS': 1,\n",
       " 'TO': 13,\n",
       " 'UH': 3,\n",
       " 'VB': 40,\n",
       " 'VBD': 10,\n",
       " 'VBG': 12,\n",
       " 'VBN': 8,\n",
       " 'VBP': 49,\n",
       " 'VBZ': 33,\n",
       " 'WP': 10,\n",
       " 'WRB': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_alt.first().pos_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
