{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicjalizacja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.param import Param\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import floor, rand, udf, max\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from post_extractor.modules.features_ import (\n",
    "    MeanFeaturesTransformer,\n",
    "    MedianFeaturesTransformer,\n",
    "    NumberOfOccurrencesFeaturesTransformer\n",
    ")\n",
    "\n",
    "sconf = SparkConf()              \\\n",
    "    .setMaster('local[*]')       \\\n",
    "    .setAppName('PipelineFlow')\n",
    "\n",
    "sc = SparkContext.getOrCreate(sconf)\n",
    "sess = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytywanie plików"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(spark_ctx, files):\n",
    "    rdd = spark_ctx.wholeTextFiles(files)\n",
    "    rdd = rdd.map(lambda x: (x[0], x[1]))\n",
    "    df = rdd.toDF(['file', 'content'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykład zastosowania TransformerProxy do automatyzacji ewaluacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformerProxy` jest obiektem który opakowuje inny Transformer. Dzięki temu, możliwe jest stworzenie uniwersalnego pipeline'u (jak na rysunku poniżej) bez specyfikowania od razu konkretnych implementacji poszczególnych etapów. Np. wiemy, że pierwszy etap parsuje plik html dzieląc go na słowa, drugi etap usuwa obrazki a trzeci rozpoznaje i zlicza czasowniki, ale nie wiemy jakie konkretne implementacje będziemy chcieli dostarczyć dla poszczególnych etapów.\n",
    " \n",
    "W szczególności jeśli bedziemy chcieli mieć wiele różnych implementacji dla tego samego etapu opisane podejście będzie użyteczne. Testowanie takiego przepływu będzie odbywało się za pomocą klasy CrossValidator która zostanie opisana później. Na razie wspomnijmy jedynie o tym, że CrossValidator nie potrafi modyfikować pipeline'u poprzez zamianę np. jednego transformera na drugi, potrafi natomiast modyfikować parametry kolejnych etapów przepływu. Dzięki zastosowanemu podejściu CrossValidator będzie w stanie testować kombinację różnych implementacji poszególnych etapów pipeline'u.\n",
    "\n",
    "![title](pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasa TransformerProxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseVectorTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    def __init__(self):\n",
    "        super(DenseVectorTransformer, self).__init__()\n",
    "    def _transform(self, dataset):\n",
    "        toDenseVector = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "        return dataset.withColumn(self.getOutputCol(), toDenseVector(self.getInputCol()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerProxy(Transformer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerProxy, self).__init__()\n",
    "        self.transformer = Param(self, \"transformer\", \"\")\n",
    "\n",
    "    def set_transformer(self, transformer):\n",
    "        self._paramMap[self.transformer] = transformer\n",
    "        return self\n",
    "\n",
    "    def get_transformer(self):\n",
    "        return self.getOrDefault(self.transformer)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return self.get_transformer().transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utworzenie instancji transformerów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przygotowanym pipeline możemy wykorzystać w pierwszym TransformerProxy trzy różne implementacje: \n",
    "- MeanFeaturesTransformer \n",
    "- MedianFeaturesTransformer \n",
    "- NumerOfOccurrencesFeaturesTransformer \n",
    "\n",
    "Możemy je zatem przekazać do abstrakcji ParamGridBuilder'a, który będzie parametrem przekazanym do klasy CrossValidator.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(MeanFeaturesTransformer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(MedianFeaturesTransformer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(NumberOfOccurrencesFeaturesTransformer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    \"features1\": [\n",
    "        \"leaf\",\n",
    "        \"has-attribute-class\",\n",
    "    ],\n",
    "    \"features2\": [\n",
    "        \"contains-adjectives\",\n",
    "        \"contains-date\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "content_column='content'\n",
    "metrics_column='metric'\n",
    "\n",
    "tf = [MeanFeaturesTransformer, MedianFeaturesTransformer, NumberOfOccurrencesFeaturesTransformer]\n",
    "\n",
    "features_transfomers = {featureSetName : [transformer(features=features, inputCol=content_column, \n",
    "                                                       outputCol=metrics_column)\n",
    "                   for transformer\n",
    "                   in tf] for featureSetName, features in features.items()}\n",
    "\n",
    "\n",
    "dv_transformer = DenseVectorTransformer()\n",
    "dv_transformer.setInputCol(metrics_column).setOutputCol('features')\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol='label')\n",
    "\n",
    "lr = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie przestrzeni parametrów \n",
    "\n",
    "Każdy transformer w pipelinie stanowi osobny wymiar w przestrzeni parametrów.\n",
    "W naszym przypadku grid ma tylko jeden wymiar o długości 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_stage_proxy = TransformerProxy()\n",
    "\n",
    "param_grids = {}\n",
    "\n",
    "for featuresSetName, transformersList in features_transfomers.items():\n",
    "    param_grid_builder = ParamGridBuilder()\n",
    "    param_grid_builder.addGrid(metric_stage_proxy.transformer, transformersList)\n",
    "    param_grid = param_grid_builder.build()\n",
    "    param_grids[featuresSetName] = param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie modyfikowalnego pipeline'u\n",
    "W tej wersji, wszystkie istniejące wczesniej stage zastępujemy obiektami `TransformerProxy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterized_pipeline = Pipeline(stages=[metric_stage_proxy, dv_transformer, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file = 'external/data/'\n",
    "loaded_features = load_features(sc, feature_file)\n",
    "dataWithLabels = loaded_features.withColumn('label', floor(rand() * 3).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossValidator uwzględniając wszystkie kombinacje dostarczonych parametrów wskazuje który zestaw parametrów cechuje się najlepszymi wynikami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidator will automatically find the best set of parameters\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='label')\n",
    "\n",
    "cv_results = {} \n",
    "\n",
    "for featuresSetName, param_grid in param_grids.items():\n",
    "    cv = CrossValidator(estimator=parameterized_pipeline,\n",
    "                       estimatorParamMaps=param_grid,\n",
    "                       evaluator=evaluator,\n",
    "                       numFolds=3)\n",
    "    cv_result = cv.fit(dataWithLabels)\n",
    "    cv_results[featuresSetName] = cv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wypisanie nazw transformerow wybranych przez CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected transformers:\n",
      "MeanFeaturesTransformer\n"
     ]
    }
   ],
   "source": [
    "def display_results(cv_results):\n",
    "    bestModels = {}\n",
    "    for featureSetName, cv_result in cv_results.items():\n",
    "        model_metric_pairs = zip(cv_result.avgMetrics, cv_result.getEstimatorParamMaps())\n",
    "        bestModels[featureSetName] = sorted(model_metric_pairs, key=lambda x: x[0])[0]\n",
    " \n",
    "    model_metric_pairs = sorted(bestModels.items(), key=lambda x: -x[1][0])\n",
    "    best_model = model_metric_pairs[0]\n",
    " \n",
    "    feature_set_name, best_metric_model_pair = best_model\n",
    "    metric, model = best_metric_model_pair\n",
    "    print(\"Best model transformers (feature set %s): \" % feature_set_name)\n",
    "    for pipeline_param, transformer in model.items():\n",
    "      print(type(transformer).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(cv_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "name": "PipelineCV_nofilter",
  "notebookId": 4.28779215948259E14
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
