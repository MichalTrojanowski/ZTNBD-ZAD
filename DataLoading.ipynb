{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, rootDir):\n",
    "        self.rootDir = rootDir\n",
    "        \n",
    "    def loadData(self, sparkContext):\n",
    "        pass\n",
    "\n",
    "class DataLoader(Loader):\n",
    "    key = 'key'\n",
    "    content = 'content'\n",
    "    path = 'path'\n",
    "    \n",
    "    def loadData(self, sc):\n",
    "        def extractKey(entry):\n",
    "            fpath, content = entry\n",
    "            fname = fpath.split('/')[-1]\n",
    "            key = re.sub('\\.json$', '', fname)\n",
    "            return (key, json.loads(content)) #json parsing should be part of transformer code\n",
    "        \n",
    "        mappings = sc.textFile(self.rootDir + 'mapping.csv')        \\\n",
    "            .map(lambda line: line.split('|'))                      \\\n",
    "            .toDF([self.path, self.key])\n",
    "        \n",
    "        statements = sc.wholeTextFiles(self.rootDir + 'statement/') \\\n",
    "            .map(extractKey)                                        \\\n",
    "            .toDF([self.key, self.content])\n",
    "    \n",
    "        return mappings.join(statements, 'key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaimportowanie modułów przygotowanych przez drugą grupę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from post_extractor.modules.posts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Definicja źródła danych (przykładowa klasa)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = DataLoader('./kascysko.blogspot.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Defnicja przepływu*\n",
    "Przepływ definiowany jest poprzez stworzenie instancji kolejnych metod i przekazanie im kolumny (bądź kolumn) wejściowej i specyfikacja wyjścia każdej z metod. Następnie tworzony jest obiekt `pipeline` grupujący kolejne transformery w przepływ. Istotne jest aby zachować odpowiednią kolejność podczas przekazywania metod do pipeline, powinna ona być zgodna z zależnościami między metodami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "poster = PostTransformer()                  \\\n",
    "    .setInputCol(source.content)            \\\n",
    "    .setOutputCol('posts')\n",
    "\n",
    "translator = TranslateTransformer()         \\\n",
    "    .setInputCol(poster.getOutputCol())     \\\n",
    "    .setOutputCol('translated')\n",
    "        \n",
    "sentencer = SentenceTransformer()           \\\n",
    "    .setInputCol(translator.getOutputCol()) \\\n",
    "    .setOutputCol('sentences')              \n",
    "\n",
    "pipeline = Pipeline(stages=[poster, translator, sentencer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - uruchomienie Sparka oraz odczyt danych z dysku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sconf = SparkConf()              \\\n",
    "    .setMaster('local[*]')       \\\n",
    "    .setAppName('PipelineFlow')\n",
    "\n",
    "sc = SparkContext.getOrCreate(sconf)\n",
    "df = source.loadData(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uruchomienie przepływu i odczyt danych\n",
    "Po dopasowaniu parametrów metod składających się na przepływ otrzymujemy model, który może być później zapisany dzieki czemu następnym razem nie będzie konieczności dobierania parametrów dla nowych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(df)\n",
    "output = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I Jantar spiced with yeast beer :) described wcierka for me a bit too expensive',\n",
       " 'I have banyan water with isany, but on holiday I am not very well with regularity.',\n",
       " 'Jantar is waiting for autumn.',\n",
       " 'I have this wcierkę on wishliscie but is piekielnie droga: /',\n",
       " 'I like to use all wcierki :)']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.first().sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmiana przepływu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counter = SpeechPartsTransformer()       \\\n",
    "    .setInputCol(translator.getOutputCol())  \\\n",
    "    .setOutputCol('pos_count')\n",
    "\n",
    "pipeline_alt = Pipeline(stages=[poster, translator, pos_counter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_alt = pipeline_alt.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CC': 28,\n",
       " 'CD': 6,\n",
       " 'DT': 69,\n",
       " 'EX': 2,\n",
       " 'IN': 107,\n",
       " 'JJ': 57,\n",
       " 'JJR': 3,\n",
       " 'JJS': 2,\n",
       " 'MD': 19,\n",
       " 'NN': 138,\n",
       " 'NNP': 23,\n",
       " 'NNS': 32,\n",
       " 'PDT': 1,\n",
       " 'PRP': 105,\n",
       " 'PRP$': 12,\n",
       " 'RB': 66,\n",
       " 'RBR': 3,\n",
       " 'RBS': 1,\n",
       " 'SYM': 1,\n",
       " 'TO': 13,\n",
       " 'UH': 2,\n",
       " 'VB': 40,\n",
       " 'VBD': 10,\n",
       " 'VBG': 12,\n",
       " 'VBN': 8,\n",
       " 'VBP': 49,\n",
       " 'VBZ': 32,\n",
       " 'WP': 10,\n",
       " 'WRB': 5}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_alt.first().pos_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
