{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uwaga\n",
    "Wersja zgodna z masterem z repo post_extractor. Przed odpaleniem trzeba zrobiÄ‡ checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['SPARK_HOME'] = 'C:\\\\Users\\\\Mateusz\\\\Downloads\\\\Spark\\\\spark-2.2.1-bin-hadoop2.7'\n",
    "# os.environ['HADOOP_HOME'] = os.environ['PWD']\n",
    "import findspark\n",
    "findspark.init('/home/marta/Pobrane/spark-2.2.1-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import ArrayType, IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "from post_extractor.modules.posts import (\n",
    "    SentenceTransformer,\n",
    "    PostTransformer,\n",
    "    TranslateTransformer,\n",
    "    SpeechPartsTransformer,\n",
    "    SentimentTransformer\n",
    ")\n",
    "from post_extractor.modules.features_ import (\n",
    "    FeatureTransformer\n",
    ")\n",
    "\n",
    "from post_extractor.modules.universal import (\n",
    "    ConvertDictToVectorTransformer,\n",
    "    SelectRecordsTransformer,\n",
    "    MaxTransformer,\n",
    "    MeanTransformer,\n",
    "    MedianTransformer,\n",
    "    NumberOfOccurrencesTransformer,\n",
    ")\n",
    "\n",
    "sconf = SparkConf()              \\\n",
    "    .setMaster('local[*]')       \\\n",
    "    .setAppName('PipelineFlow')\n",
    "\n",
    "sc = SparkContext.getOrCreate(sconf)\n",
    "sess = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.param import Param\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "class TransformerProxy(Transformer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerProxy, self).__init__()\n",
    "        self.transformer = Param(self, \"transformer\", \"\")\n",
    "\n",
    "    def set_transformer(self, transformer):\n",
    "        self._paramMap[self.transformer] = transformer\n",
    "        return self\n",
    "\n",
    "    def get_transformer(self):\n",
    "        return self.getOrDefault(self.transformer)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return self.get_transformer().transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Estimator\n",
    "\n",
    "class EstimatorProxy(Estimator):\n",
    "    def __init__(self):\n",
    "        super(EstimatorProxy, self).__init__()\n",
    "        self.estimator = Param(self, \"estimator\", \"\")\n",
    "\n",
    "    def set_estimator(self, estimator):\n",
    "        self._paramMap[self.estimator] = estimator\n",
    "        return self\n",
    "\n",
    "    def get_estimator(self):\n",
    "        return self.getOrDefault(self.estimator)\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        return self.get_estimator().fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimators_grid(estimator, param_grid):\n",
    "    result = []\n",
    "    for param_map in param_grid:\n",
    "        est_copy = estimator.copy()\n",
    "        est_copy.setParams(**param_map)\n",
    "        result.append(est_copy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark_ctx, root):\n",
    "    posts_rdd = spark_ctx.wholeTextFiles(root + 'posts')\n",
    "    posts_rdd = posts_rdd.map(lambda x: (x[0].split('/')[-1].rstrip('.json'), json.loads(x[1])))\n",
    "    posts_df = posts_rdd.toDF(['key', 'content_post'])\n",
    "\n",
    "    features_rdd = spark_ctx.wholeTextFiles(root + 'features')\n",
    "    features_rdd = features_rdd.map(lambda x: (x[0].split('/')[-1].rstrip('.features'), x[1]))\n",
    "    features_df = features_rdd.toDF(['key', 'content_features'])\n",
    "    \n",
    "    features_df.show()\n",
    "    return posts_df.join(features_df, 'key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 key|    content_features|\n",
      "+--------------------+--------------------+\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                 key|        content_post|    content_features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|kascysko.blogspot...|[Map(sourceURL ->...|{\"path\":[1,2,0],\"...|\n",
      "|kascysko.blogspot...|[Map(sourceURL ->...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|[Map(sourceURL ->...|{\"path\":[1,4,1,1,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_data(sc, 'data/').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_choices = [[\"leaf\", \"has-attribute-class\",], [\"contains-adjectives\", \"contains-date\"]]\n",
    "\n",
    "featurer = FeatureTransformer();\n",
    "featurer.setInputCol('content_features').setOutputCol('features')\n",
    "\n",
    "feature_selector = SelectRecordsTransformer(keys=features_choices[0], element_type=ArrayType(DoubleType()))\n",
    "feature_selector.setInputCol(featurer.getOutputCol()).setOutputCol('selected_features')\n",
    "\n",
    "aggregated_features = 'aggregated_features'\n",
    "\n",
    "max_feature_transformer = MaxTransformer()\n",
    "max_feature_transformer.setInputCol(feature_selector.getOutputCol()).setOutputCol(aggregated_features)\n",
    "\n",
    "mean_feature_transformer = MeanTransformer()\n",
    "mean_feature_transformer.setInputCol(feature_selector.getOutputCol()).setOutputCol(aggregated_features)\n",
    "\n",
    "median_feature_transformer = MedianTransformer()\n",
    "median_feature_transformer.setInputCol(feature_selector.getOutputCol()).setOutputCol(aggregated_features)\n",
    "\n",
    "number_of_occurences_feature_transformer = NumberOfOccurrencesTransformer()\n",
    "number_of_occurences_feature_transformer.setInputCol(feature_selector.getOutputCol()).setOutputCol(aggregated_features)\n",
    "\n",
    "feature_aggregation_proxy = TransformerProxy()\n",
    "feature_aggregation_transformers = [\n",
    "    max_feature_transformer,\n",
    "    mean_feature_transformer,\n",
    "    median_feature_transformer,\n",
    "    number_of_occurences_feature_transformer,\n",
    "]\n",
    "\n",
    "features_dict_to_list_converter = ConvertDictToVectorTransformer(keys=features_choices[0])\n",
    "features_dict_to_list_converter.setInputCol(aggregated_features).setOutputCol('features_from_file')\n",
    "\n",
    "features_stages = [\n",
    "    featurer,\n",
    "    feature_selector,\n",
    "    feature_aggregation_proxy,\n",
    "    features_dict_to_list_converter\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "poster = PostTransformer()\n",
    "poster.setInputCol('content_post').setOutputCol('posts')\n",
    "\n",
    "translator = TranslateTransformer()\n",
    "translator.setInputCol('posts').setOutputCol('translated')\n",
    "\n",
    "sentencer = SentenceTransformer()\n",
    "sentencer.setInputCol('translated').setOutputCol('sentences')\n",
    "\n",
    "speech_parter = SpeechPartsTransformer()\n",
    "speech_parter.setInputCol('translated').setOutputCol('speech_parts')\n",
    "\n",
    "sentimenter = SentimentTransformer()\n",
    "sentimenter.setInputCol('translated').setOutputCol('sentiments')\n",
    "\n",
    "tags = [\n",
    "    'NN',\n",
    "    'NNS',\n",
    "    'NNPS'\n",
    "]\n",
    "\n",
    "aggregated_nouns_col = 'aggregated_nouns'\n",
    "nouns_col = 'nouns'\n",
    "\n",
    "speech_parts_selector = SelectRecordsTransformer(keys=tags, element_type=ArrayType(IntegerType()))\n",
    "speech_parts_selector.setInputCol(speech_parter.getOutputCol()).setOutputCol('nouns')\n",
    "\n",
    "max_nouns_transformer = MaxTransformer()\n",
    "max_nouns_transformer.setInputCol(speech_parts_selector.getOutputCol()).setOutputCol(aggregated_nouns_col)\n",
    "\n",
    "mean_nouns_transformer = MeanTransformer()\n",
    "mean_nouns_transformer.setInputCol(speech_parts_selector.getOutputCol()).setOutputCol(aggregated_nouns_col)\n",
    "\n",
    "median_nouns_transformer = MedianTransformer()\n",
    "median_nouns_transformer.setInputCol(speech_parts_selector.getOutputCol()).setOutputCol(aggregated_nouns_col)\n",
    "\n",
    "post_aggregation_proxy = TransformerProxy()\n",
    "post_aggregation_transformers = [max_nouns_transformer, mean_nouns_transformer, median_nouns_transformer]\n",
    "\n",
    "posts_dict_to_list_converter = ConvertDictToVectorTransformer(keys=tags)\n",
    "posts_dict_to_list_converter.setInputCol(aggregated_nouns_col).setOutputCol('post_features')\n",
    "\n",
    "post_stages = [\n",
    "    poster,\n",
    "    translator, \n",
    "    sentencer, \n",
    "    speech_parter,\n",
    "    sentimenter,\n",
    "    speech_parts_selector,\n",
    "    post_aggregation_proxy,\n",
    "    posts_dict_to_list_converter\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "class DenseVectorTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    def __init__(self):\n",
    "        super(DenseVectorTransformer, self).__init__()\n",
    "    def _transform(self, dataset):\n",
    "        toDenseVector = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "        return dataset.withColumn(self.getOutputCol(), toDenseVector(self.getInputCol()))\n",
    "    \n",
    "features_dv = DenseVectorTransformer().setInputCol(features_dict_to_list_converter.getOutputCol()).setOutputCol('features_dv')\n",
    "posts_dv = DenseVectorTransformer().setInputCol(posts_dict_to_list_converter.getOutputCol()).setOutputCol('posts_dv')\n",
    "hotfix_stages = [features_dv, posts_dv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    features_dict_to_list_converter.getOutputCol(),\n",
    "    posts_dict_to_list_converter.getOutputCol()\n",
    "]\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=all_features, outputCol='feature_vector')\n",
    "\n",
    "#classifier = DecisionTreeClassifier(featuresCol=vector_assembler.getOutputCol())\n",
    "\n",
    "#classification_stages = [vector_assembler, classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_join_stages = [vector_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'label'\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(featuresCol=vector_assembler.getOutputCol(), labelCol=label_col)\n",
    "dtParamGrid = ParamGridBuilder() \\\n",
    "  .baseOn({\"labelCol\": label_col}) \\\n",
    "  .addGrid(\"maxDepth\", [2, 3]) \\\n",
    "  .addGrid(\"maxBins\", [5, 10]) \\\n",
    "  .build()\n",
    "  \n",
    "estimators_grid = create_estimators_grid(decision_tree_classifier, dtParamGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lrc = LogisticRegression(featuresCol=vector_assembler.getOutputCol(), labelCol=label_col)\n",
    "lrParamGrid = ParamGridBuilder() \\\n",
    "  .baseOn({\"labelCol\": label_col}) \\\n",
    "  .addGrid(\"maxIter\", [10, 50]) \\\n",
    "  .addGrid(\"regParam\", [0, 0.1]) \\\n",
    "  .build()\n",
    "  \n",
    "estimators_grid += create_estimators_grid(lrc, lrParamGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_proxy = EstimatorProxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = features_stages + post_stages + pipeline_join_stages + [est_proxy])\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(feature_aggregation_proxy.transformer, feature_aggregation_transformers) \\\n",
    "    .addGrid(post_aggregation_proxy.transformer, post_aggregation_transformers) \\\n",
    "    .addGrid(est_proxy.estimator, estimators_grid) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validator = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=param_grid, \n",
    "    evaluator=evaluator)\n",
    "# add numFolds = 3 if CV crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 key|    content_features|\n",
      "+--------------------+--------------------+\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "|kascysko.blogspot...|{\"path\":[1,4,1,1,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, floor\n",
    "input_data = load_data(sc, 'data/')\n",
    "labeled_data = input_data.withColumn('label', floor(rand() * 3).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = cross_validator.fit(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------------+\n",
      "|key|content_post|content_features|\n",
      "+---+------------+----------------+\n",
      "+---+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cv_result.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = prediction.select(\"file\", \"speech_parts\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = cv_result.bestModel.stages[-1]\n",
    "print(\"Chosen estimator: \", type(estimator).__name__)\n",
    "print(estimator._java_obj.extractParamMap().toString().split(r\"\\n\\t\")[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "name": "PipelineAll_mntss",
  "notebookId": 31331438541056
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
