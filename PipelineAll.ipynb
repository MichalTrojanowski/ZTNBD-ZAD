{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uwaga\n",
    "Wersja zgodna z masterem z repo post_extractor. Przed odpaleniem trzeba zrobiÄ‡ checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Mateusz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import ArrayType, IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "from post_extractor.modules.posts import (\n",
    "    SentenceTransformer,\n",
    "    PostTransformer,\n",
    "    TranslateTransformer,\n",
    "    SpeechPartsTransformer,\n",
    "    SentimentTransformer\n",
    ")\n",
    "from post_extractor.modules.features_ import (\n",
    "    FeatureTransformer\n",
    ")\n",
    "from post_extractor.modules.universal import (\n",
    "    ConvertDictToListTransformer,\n",
    "    SelectRecordsTransformer,\n",
    "    MaxTransformer,\n",
    "    MeanTransformer,\n",
    "    MedianTransformer,\n",
    "    NumberOfOccurrencesTransformer,\n",
    ")\n",
    "\n",
    "sconf = SparkConf()              \\\n",
    "    .setMaster('local[*]')       \\\n",
    "    .setAppName('PipelineFlow')\n",
    "\n",
    "sc = SparkContext.getOrCreate(sconf)\n",
    "sess = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.param import Param\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml import Transformer\n",
    "class TransformerProxy(Transformer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerProxy, self).__init__()\n",
    "        self.transformer = Param(self, \"transformer\", \"\")\n",
    "\n",
    "    def set_transformer(self, transformer):\n",
    "        self._paramMap[self.transformer] = transformer\n",
    "        return self\n",
    "\n",
    "    def get_transformer(self):\n",
    "        return self.getOrDefault(self.transformer)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return self.get_transformer().transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark_ctx, root):\n",
    "    posts_rdd = spark_ctx.wholeTextFiles(root + 'posts')\n",
    "    posts_rdd = posts_rdd.map(lambda x: (x[0].split('/')[-1].rstrip('.json'), json.loads(x[1])))\n",
    "    posts_df = posts_rdd.toDF(['key', 'content_post'])\n",
    "\n",
    "    features_rdd = spark_ctx.wholeTextFiles(root + 'features')\n",
    "    features_rdd = features_rdd.map(lambda x: (x[0].split('/')[-1].rstrip('.features'), x[1]))\n",
    "    features_df = features_rdd.toDF(['key', 'content_features'])\n",
    "    \n",
    "    \n",
    "    return posts_df.join(features_df, 'key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_choices = [[\"leaf\", \"has-attribute-class\",], [\"contains-adjectives\", \"contains-date\"]]\n",
    "\n",
    "featurer = FeatureTransformer();\n",
    "featurer.setInputCol('content_features').setOutputCol('features')\n",
    "\n",
    "feature_selector = SelectRecordsTransformer(keys=features_choices[0], element_type=ArrayType(DoubleType()))\n",
    "feature_selector.setInputCol(featurer.getOutputCol()).setOutputCol('selected_features')\n",
    "\n",
    "aggregated_features = 'aggregated_features'\n",
    "\n",
    "max_feature_transformer = MaxTransformer()\n",
    "max_feature_transformer.setInputCol(feature_selector.getOutputCol()).setOutputCol(aggregated_features)\n",
    "\n",
    "mean_feature_transformer = MeanTransformer()\n",
    "mean_feature_transformer.setInputCol(feature_selector.getOutputCol()).setOutputCol(aggregated_features)\n",
    "\n",
    "median_feature_transformer = MedianTransformer()\n",
    "median_feature_transformer.setInputCol(feature_selector.getOutputCol()).setOutputCol(aggregated_features)\n",
    "\n",
    "number_of_occurences_feature_transformer = NumberOfOccurrencesTransformer()\n",
    "number_of_occurences_feature_transformer.setInputCol(feature_selector.getOutputCol()).setOutputCol(aggregated_features)\n",
    "\n",
    "feature_aggregation_proxy = TransformerProxy()\n",
    "feature_aggregation_transformers = [\n",
    "    max_feature_transformer,\n",
    "    mean_feature_transformer,\n",
    "    median_feature_transformer,\n",
    "    number_of_occurences_feature_transformer,\n",
    "]\n",
    "\n",
    "features_dict_to_list_converter = ConvertDictToListTransformer(keys=features_choices[0], element_type=DoubleType())\n",
    "features_dict_to_list_converter.setInputCol(aggregated_features).setOutputCol('features_from_file')\n",
    "\n",
    "features_stages = [\n",
    "    featurer,\n",
    "    feature_selector,\n",
    "    feature_aggregation_proxy,\n",
    "    features_dict_to_list_converter\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "poster = PostTransformer()\n",
    "poster.setInputCol('content_post').setOutputCol('posts')\n",
    "\n",
    "translator = TranslateTransformer()\n",
    "translator.setInputCol('posts').setOutputCol('translated')\n",
    "\n",
    "sentencer = SentenceTransformer()\n",
    "sentencer.setInputCol('translated').setOutputCol('sentences')\n",
    "\n",
    "speech_parter = SpeechPartsTransformer()\n",
    "speech_parter.setInputCol('translated').setOutputCol('speechParts')\n",
    "\n",
    "sentimenter = SentimentTransformer()\n",
    "sentimenter.setInputCol('translated').setOutputCol('sentiments')\n",
    "\n",
    "tags = [\n",
    "    'NN',\n",
    "    'NNS',\n",
    "    'NNPS'\n",
    "]\n",
    "\n",
    "aggregated_nouns_col = 'aggregated_nouns'\n",
    "nouns_col = 'nouns'\n",
    "\n",
    "speech_parts_selector = SelectRecordsTransformer(keys=tags, element_type=ArrayType(IntegerType()))\n",
    "speech_parts_selector.setInputCol(speech_parter.getOutputCol()).setOutputCol('nouns')\n",
    "\n",
    "max_nouns_transformer = MaxTransformer()\n",
    "max_nouns_transformer.setInputCol(speech_parts_selector.getOutputCol()).setOutputCol(aggregated_nouns_col)\n",
    "\n",
    "mean_nouns_transformer = MeanTransformer()\n",
    "mean_nouns_transformer.setInputCol(speech_parts_selector.getOutputCol()).setOutputCol(aggregated_nouns_col)\n",
    "\n",
    "median_nouns_transformer = MedianTransformer()\n",
    "median_nouns_transformer.setInputCol(speech_parts_selector.getOutputCol()).setOutputCol(aggregated_nouns_col)\n",
    "\n",
    "post_aggregation_proxy = TransformerProxy()\n",
    "post_aggregation_transformers = [max_nouns_transformer, mean_nouns_transformer, median_nouns_transformer]\n",
    "\n",
    "posts_dict_to_list_converter = ConvertDictToListTransformer(keys=tags, element_type=DoubleType())\n",
    "posts_dict_to_list_converter.setInputCol(aggregated_nouns_col).setOutputCol('post_features')\n",
    "\n",
    "post_stages = [\n",
    "    poster,\n",
    "    translator, \n",
    "    sentencer, \n",
    "    speech_parter,\n",
    "    sentimenter,\n",
    "    speech_parts_selector,\n",
    "    post_aggregation_proxy,\n",
    "    posts_dict_to_list_converter\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "class DenseVectorTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    def __init__(self):\n",
    "        super(DenseVectorTransformer, self).__init__()\n",
    "    def _transform(self, dataset):\n",
    "        toDenseVector = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "        return dataset.withColumn(self.getOutputCol(), toDenseVector(self.getInputCol()))\n",
    "    \n",
    "features_dv = DenseVectorTransformer().setInputCol(features_dict_to_list_converter.getOutputCol()).setOutputCol('features_dv')\n",
    "posts_dv = DenseVectorTransformer().setInputCol(posts_dict_to_list_converter.getOutputCol()).setOutputCol('posts_dv')\n",
    "hotfix_stages = [features_dv, posts_dv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    features_dv.getOutputCol(),\n",
    "    posts_dv.getOutputCol()\n",
    "]\n",
    "vector_assembler = VectorAssembler(inputCols=all_features, outputCol='feature_vector')\n",
    "\n",
    "classifier = DecisionTreeClassifier(featuresCol=vector_assembler.getOutputCol())\n",
    "\n",
    "classification_stages = [vector_assembler, classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = features_stages + post_stages + hotfix_stages + classification_stages)\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(feature_aggregation_proxy.transformer, feature_aggregation_transformers) \\\n",
    "    .addGrid(post_aggregation_proxy.transformer, post_aggregation_transformers) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validator = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=param_grid, \n",
    "    evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, floor\n",
    "input_data = load_data(sc, 'data/').withColumn('label', floor(rand() * 3).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validator.fit(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
